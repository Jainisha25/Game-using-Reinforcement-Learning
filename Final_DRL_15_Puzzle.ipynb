{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEjuTx7klpWD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class N_puzzle():\n",
        "    def __init__(self,n, difficulty = 5):\n",
        "        #the n is the dimension of the puzzle\n",
        "        #the difficulty is the manhattan distance of the starting puzzle\n",
        "        assert type(n) is int and n > 1\n",
        "        self.n = n\n",
        "        self.N = n**2\n",
        "        #the reason to use torch.tensor here is that it is convenient to input the model\n",
        "        board = torch.zeros((self.N,self.N - 1)).int()\n",
        "        for i in range(self.N - 1):\n",
        "            board[i,i] = 1\n",
        "        self.board = board.view(n,n,-1)\n",
        "        #the final is the goal to be achieved\n",
        "        self.final = copy.deepcopy(self.board)\n",
        "        #a dict to convert string into direction represented by int\n",
        "        self.way_dict = {\"up\":0, \"down\":1, \"left\":2, \"right\":3}\n",
        "        #convert the direction into vector\n",
        "        self.direction_list = [(1,0),(-1,0),(0,-1),(0,1)]\n",
        "        #zero_loc traces the position of the zero or empty\n",
        "        self.zero_loc = np.array([n-1, n-1])\n",
        "        #the mark is used to convert the one hot-key puzzle to the visible puzzle\n",
        "        self.mark = torch.arange(self.N - 1)+1\n",
        "        self.mark = self.mark.int().view(1,1,-1)\n",
        "        #initilize the puzzle\n",
        "        self.random_walk(difficulty)\n",
        "\n",
        "    # the move will return reward based on the movement:\n",
        "    # achieve the goal: 10, hit the wall:-1, otherwise: 0\n",
        "    def move(self, way):\n",
        "        if isinstance(way, str) and way in self.way_dict:\n",
        "            way = self.way_dict[way]\n",
        "        assert isinstance(way, int) and 0 <= way <= 3, \"Please input an integer between 0 and 3\"\n",
        "        direction = self.direction_list[way]\n",
        "        new_loc = self.zero_loc + direction\n",
        "\n",
        "        if np.any(new_loc >= self.n) or np.any(new_loc < 0):\n",
        "            return -1  # Hit the wall\n",
        "\n",
        "        self.swap(self.zero_loc, new_loc)\n",
        "        self.zero_loc = new_loc\n",
        "\n",
        "        if self.achieve_final():\n",
        "            return 10  # Achieve the goal\n",
        "        else:\n",
        "            return 0  # Otherwise\n",
        "\n",
        "\n",
        "    def swap(self, old, new):\n",
        "        tmp = self.board[old[0],old[1]].clone()\n",
        "        self.board[old[0],old[1]] = self.board[new[0],new[1]]\n",
        "        self.board[new[0],new[1]] = tmp\n",
        "\n",
        "    # This also acts as a reward to guide the puzzle\n",
        "    def manhattan(self):\n",
        "        table = self.display()\n",
        "        row = (table-1) / self.n\n",
        "        col = (table-1) % self.n\n",
        "        std = torch.arange(self.n).int()\n",
        "        d_row = (row - std.view(self.n,1)).abs().sum()\n",
        "        d_col = (col - std.view(1,self.n)).abs().sum()\n",
        "        zero_distance = ((table == 0).nonzero().int() - torch.IntTensor([0,self.n - 1])).abs().sum()\n",
        "        return d_row + d_col - zero_distance\n",
        "\n",
        "    def achieve_final(self):\n",
        "        return torch.equal(self.board, self.final)\n",
        "\n",
        "    def display(self):\n",
        "        table = self.board.int() * self.mark\n",
        "        table = table.sum(dim = 2)\n",
        "        return table\n",
        "\n",
        "    def random_walk(self, n_step):\n",
        "        distance = 0\n",
        "        while distance < n_step:\n",
        "            step = np.random.randint(4)\n",
        "            self.move(step)\n",
        "            distance = self.manhattan()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyZGRviHjRdr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import random\n",
        "# from n_puzzle import N_puzzle\n",
        "\n",
        "class hidden_unit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation):\n",
        "        super(hidden_unit, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.linear = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "#Here I use the linear model as the network\n",
        "class Q_learning(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_layers, out_channels = 4, unit = hidden_unit, activation = F.relu):\n",
        "        super(Q_learning, self).__init__()\n",
        "        assert type(hidden_layers) is list\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_units = nn.ModuleList()\n",
        "        prev_layer = in_channels\n",
        "        for hidden in hidden_layers:\n",
        "            self.hidden_units.append(unit(prev_layer, hidden, activation))\n",
        "            prev_layer = hidden\n",
        "        self.final_unit = nn.Linear(prev_layer, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.view(-1, self.in_channels).float()\n",
        "        for unit in self.hidden_units:\n",
        "            out = unit(out)\n",
        "        out = self.final_unit(out)\n",
        "        return out\n",
        "\n",
        "# The transition and ReplayMemory are copied from the website:\n",
        "# http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'new_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        #\"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "#Just follow the object-oriented coding style...use a class to train and test\n",
        "class RL_training():\n",
        "    def __init__(self, N, difficulty, Q_learning, epsilon = 1, gamma = 0.5, lr = 0.1, buffer = 100, batch_size = 40):\n",
        "        self.N = N\n",
        "        self.difficulty = difficulty\n",
        "        self.N_puzzle = N_puzzle(self.N, difficulty)\n",
        "        self.model = Q_learning\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.optimizer = optim.RMSprop(self.model.parameters(), lr = lr)\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.buffer = buffer\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = ReplayMemory(buffer)\n",
        "    def train(self, epochs = 5001, record_sections = 1000):\n",
        "        #to count the success rate in one record_section\n",
        "        sucess_count = 0\n",
        "        for i in range(epochs):\n",
        "            if i%record_sections ==0:\n",
        "                print('\\n')\n",
        "                print(\"epoch is %d\" %(i))\n",
        "                print(\"epsilon is %.2f\" % self.epsilon)\n",
        "                print(\"success rate: %.2f\" % (sucess_count/record_sections*100))\n",
        "                sucess_count = 0\n",
        "            self.N_puzzle = N_puzzle(self.N, self.difficulty)\n",
        "            step = 0\n",
        "            while True:\n",
        "                #get the current state\n",
        "                state = Variable(self.N_puzzle.board.clone().view(1,-1))\n",
        "                #get the current Q values\n",
        "                qval = self.model.forward(state)\n",
        "                if (np.random.random() < self.epsilon):\n",
        "                    #choose random action\n",
        "                    action = np.random.randint(0,4)\n",
        "                else:\n",
        "                    #choose best action from Q(s,a) values\n",
        "                    action = np.argmax(qval.data)\n",
        "                #Take action, get the reward\n",
        "                # print(state)\n",
        "                # print(action)\n",
        "                # data = input(\"mark.\")\n",
        "                # print(data)\n",
        "                # reward = self.N_puzzle.move(action) - self.N_puzzle.manhattan()\n",
        "                reward = self.N_puzzle.move(int(action)) - self.N_puzzle.manhattan()\n",
        "                #Acuqire new state\n",
        "                new_state = Variable(self.N_puzzle.board.clone().view(1,-1))\n",
        "                #push the state-pair into memory\n",
        "                self.memory.push(state.data, action, new_state.data, reward)\n",
        "                step += 1\n",
        "                # If the memory is not full, skip the update part\n",
        "                if (len(self.memory) < self.buffer): #if buffer not filled, add to it\n",
        "                    state = new_state\n",
        "                    if reward == 10: #if reached terminal state, update game status\n",
        "                        break\n",
        "                    elif step >= 20:\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "                #sample a batch of state-pairs\n",
        "                transitions = self.memory.sample(self.batch_size)\n",
        "                batch = Transition(*zip(*transitions))\n",
        "                state_batch = Variable(torch.cat(batch.state))\n",
        "                action_batch = Variable(torch.LongTensor(batch.action)).view(-1,1)\n",
        "                # new_state_batch = Variable(torch.cat(batch.new_state), volatile = True)\n",
        "                with torch.no_grad():\n",
        "                    new_state_batch = Variable(torch.cat(batch.new_state))\n",
        "                reward_batch = Variable(torch.FloatTensor(batch.reward))\n",
        "                # the non_final_mask is to determine the reward\n",
        "                # If a state achieves the goal, it should only get the reward, not reward + QMax\n",
        "                # Because there is no future state for it.\n",
        "                non_final_mask = (reward_batch != 10)\n",
        "                #Let's run our Q function on S to get Q values for all possible actions\n",
        "                # we only update the qval[action], leaving qval[not action] unchanged\n",
        "                state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "                #new Q\n",
        "                newQ = self.model(new_state_batch)\n",
        "                maxQ = newQ.max(1)[0]\n",
        "                #update the y\n",
        "#                 if reward == 0: #non-terminal state\n",
        "#                     update = (-1.0*distance + (self.gamma * maxQ.data))\n",
        "#                 else: #terminal state\n",
        "#                     update = reward\n",
        "                y = reward_batch\n",
        "                y[non_final_mask] += self.gamma * maxQ[non_final_mask]\n",
        "                #the y does not need to be back-propogated, so set to be not volatile.\n",
        "                # y.volatile = False\n",
        "                # With this line\n",
        "                with torch.no_grad():\n",
        "                    y_detached = y.detach().clone().requires_grad_(True)\n",
        "                # loss = self.criterion(state_action_values, y)\n",
        "                target = y.view(-1, 1)  # or target = y.unsqueeze(1)\n",
        "                loss = self.criterion(state_action_values, target)\n",
        "                # Optimize the model\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                for param in self.model.parameters():\n",
        "                    param.grad.data.clamp_(-1, 1)\n",
        "                self.optimizer.step()\n",
        "                # if an agent walks so many times and does not achieve the goal, it is likely that it has lost\n",
        "                # Therefore, let the agent re-try the puzzle\n",
        "                if reward == 10:\n",
        "                    if i %record_sections == 0:\n",
        "                        print(\"%d steps are trained and the reward is %d\" %(step, reward))\n",
        "                    sucess_count += 1\n",
        "                    break\n",
        "                if step >= 20:\n",
        "                    if i %record_sections == 0:\n",
        "                        print(\"%d steps are trained and stop\" % (step))\n",
        "                    break\n",
        "            if self.epsilon > 0.1:\n",
        "                self.epsilon -= (1/epochs)\n",
        "\n",
        "\n",
        "    def test(self, difficulty = None):\n",
        "            if type(difficulty) == int:\n",
        "                self.difficulty = difficulty\n",
        "            i = 0\n",
        "            print(\"Initial State:\")\n",
        "            self.N_puzzle = N_puzzle(self.N, self.difficulty)\n",
        "            print(self.N_puzzle.display())\n",
        "            status = 1\n",
        "            #while game still in progress\n",
        "            while(status == 1):\n",
        "                state = Variable(self.N_puzzle.board.clone(), requires_grad = False)\n",
        "                qval = self.model(state)\n",
        "                print(qval.data)\n",
        "                print(\"distance: \",self.N_puzzle.manhattan())\n",
        "                # action = np.argmax(qval.data) #take action with highest Q-value\n",
        "                action = int(torch.argmax(qval.data))  # Convert the tensor result to an integer\n",
        "                print('Move #: %s; Taking action: %s' % (i, action))\n",
        "                reward = self.N_puzzle.move(action)\n",
        "                print(self.N_puzzle.display())\n",
        "                if reward == 10:\n",
        "                    status = 0\n",
        "                    print(\"Reward: %s\" % (reward,))\n",
        "                i += 1 #If we're taking more than 10 actions, just stop, we probably can't win this game\n",
        "                if (i > 15):\n",
        "                    print(\"Game lost; too many moves.\")\n",
        "                    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxgq5QDOjqrf",
        "outputId": "7cc77fd4-2d63-4e79-e565-1860198ccb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "epoch is 0\n",
            "epsilon is 1.00\n",
            "success rate: 0.00\n",
            "\n",
            "\n",
            "epoch is 1000\n",
            "epsilon is 0.90\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 2000\n",
            "epsilon is 0.80\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 3000\n",
            "epsilon is 0.70\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 4000\n",
            "epsilon is 0.60\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 5000\n",
            "epsilon is 0.50\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 6000\n",
            "epsilon is 0.40\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 7000\n",
            "epsilon is 0.30\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 8000\n",
            "epsilon is 0.20\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 9000\n",
            "epsilon is 0.10\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n",
            "\n",
            "\n",
            "epoch is 10000\n",
            "epsilon is 0.10\n",
            "success rate: 0.00\n",
            "20 steps are trained and stop\n"
          ]
        }
      ],
      "source": [
        "network = Q_learning(240, [500,500], 4, hidden_unit)\n",
        "train = RL_training(4, 5, network, epsilon = 1, gamma = 0.5, lr = 1e-4)\n",
        "train.train(10001,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKPBj1_9kUaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41d1552-6f05-454c-ed23-4d099dd58609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11,  0],\n",
            "        [13, 14, 15, 12]])\n",
            "tensor([[ 2.6599, -6.3048, -7.7943, -4.6726]])\n",
            "distance:  tensor(5.)\n",
            "Move #: 0; Taking action: 0\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12],\n",
            "        [13, 14, 15,  0]])\n",
            "Reward: 10\n"
          ]
        }
      ],
      "source": [
        "train.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrPG1uBNjikl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}